{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project\n",
    "\n",
    "https://inclass.kaggle.com/c/how-many-bikes/data\n",
    "\n",
    "season - season of the record (integer, 1-4)\n",
    "1: Spring\n",
    "2: Summer\n",
    "3: Fall\n",
    "4: Winter\n",
    "\n",
    "yr - year of the record(integer, 0-1)\n",
    "0: 2011\n",
    "1: 2012\n",
    "\n",
    "wheathersit - weather situation (integer, 1-4)\n",
    "1: clear, few clouds, or partly cloudy\n",
    "2: mist (no precipitation)\n",
    "3: light rain or light snow\n",
    "4: heavy rain, hail, or snow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the data\n",
    "* How many features do you have? How are they encoded?\n",
    "* Plot histograms to visualize the distribution of your features. Should you scale or normalize them?\n",
    "* Set up a cross-validation that you will use for all your evaluations. Notice there is a 'random_state' parameter to the cross-validation methods of scikit-learn, that you can use to ensure you always get the same splits. \n",
    "* [Optional] To go one step further in ensuring a fair comparison of your algorithms, you can use multiple repeats of the cross-validation procedure (using different splits each time), and report the mean & standard deviation over the repeats of the performance obtained. If you do this, you can report standard deviations in plots by using error bars.\n",
    "* Evaluate the performance of a linear regression on your data. Which evaluation metric are you using? See http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics for help defining one.\n",
    "* Submit a linear regression predictor to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import cross_validation\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = np.loadtxt('Kaggle/train.csv', delimiter=',', skiprows=1, usecols=range(2, 13))\n",
    "y = np.loadtxt('Kaggle/train.csv', delimiter=',', skiprows=1, usecols=[14])\n",
    "z = np.loadtxt('Kaggle/test.csv', delimiter=',', skiprows=1, usecols=range(2, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "XX = np.loadtxt('Kaggle/tra.csv', delimiter=',', skiprows=1, usecols=range(2, 18))\n",
    "yy = np.loadtxt('Kaggle/tra.csv', delimiter=',', skiprows=1, usecols=[19])\n",
    "zz = np.loadtxt('Kaggle/tes.csv', delimiter=',', skiprows=1, usecols=range(2, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor train, test in kf_total:\\n    print train, '\\n', test, '\\n\\n'\\n    print train.shape\\n    print test.shape\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a stratified 10-fold cross-validation\n",
    "\n",
    "#folds = cross_validation.StratifiedKFold(y, 200, shuffle=True)\n",
    "#folds = cross_validation.train_test_split(y, test_size = 0.1, random_state=None)\n",
    "#folds = cross_validation.KFold(len(y), 10, shuffle=True)\n",
    "\n",
    "kf = cross_validation.KFold(len(y), n_folds=5, shuffle=True, random_state=4)\n",
    "\n",
    "\"\"\"\n",
    "for train, test in kf_total:\n",
    "    print train, '\\n', test, '\\n\\n'\n",
    "    print train.shape\n",
    "    print test.shape\n",
    "\"\"\"\n",
    "# http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split\n",
    "# Split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://randomforests.wordpress.com/2014/02/02/basics-of-k-fold-cross-validation-and-gridsearchcv-in-scikit-learn/\n",
    "\n",
    "get different combinations of train/test data, essentially giving you ‘more’ data for validation from your original data. The number of times you ‘switch around’ the train/test data is the number of folds. Therefore, 10-Fold Cross Validation will yield 10 sets of train/test data, 5-Fold Cross Validation will yield 5 sets, and so forth.\n",
    "\n",
    "**random_state** controls the degree of randomness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-validation functions\n",
    "\n",
    "`split the data into 10 parts\n",
    "fit on 9-parts\n",
    "test accuracy on the remaining part`\n",
    "\n",
    "This is repeated on all combinations to produce ten estimates of the accuracy of the model using the current parameter setting. Typically the mean and standard deviation of the ten scores is reported. So, if we use the setting from the previous post, we get:\n",
    "\n",
    "http://chrisstrelioff.ws/sandbox/2015/06/25/decision_trees_in_python_again_cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    \"\"\"Report top n_top parameters settings, default n_top=3.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    grid_scores -- output from grid or random search\n",
    "    n_top -- how many to report, of top models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] top parameter settings found in\n",
    "                  search\n",
    "    \"\"\"\n",
    "    top_scores = sorted(grid_scores,\n",
    "                        key=itemgetter(1),\n",
    "                        reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print((\"Mean validation score: \"\n",
    "               \"{0:.3f} (std: {1:.3f})\").format(\n",
    "               score.mean_validation_score,\n",
    "               np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "    return top_scores[0].parameters\n",
    "\n",
    "def run_gridsearch(X, y, clf, param_grid, cv=5):\n",
    "    \"\"\"Run a grid search for best Decision Tree parameters.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn Decision Tree\n",
    "    param_grid -- [dict] parameter settings to test\n",
    "    cv -- fold of cross-validation, default 5\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    clf.n_jobs = -1    \n",
    "    grid_search = GridSearchCV(clf,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=cv)\n",
    "    start = time()\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print((\"\\nGridSearchCV took {:.2f} \"\n",
    "           \"seconds for {:d} candidate \"\n",
    "           \"parameter settings.\").format(time() - start,\n",
    "                len(grid_search.grid_scores_)))\n",
    "\n",
    "    top_params = report(grid_search.grid_scores_, 3)\n",
    "    return  top_params\n",
    "\n",
    "\n",
    "def run_randomsearch(X, y, clf, para_dist, cv=5,\n",
    "                     n_iter_search=20):\n",
    "    \"\"\"Run a random search for best Decision Tree parameters.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn Decision Tree\n",
    "    param_dist -- [dict] list, distributions of parameters\n",
    "                  to sample\n",
    "    cv -- fold of cross-validation, default 5\n",
    "    n_iter_search -- number of random parameter sets to try,\n",
    "                     default 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    clf.n_jobs = -1    \n",
    "    random_search = RandomizedSearchCV(clf,\n",
    "                        param_distributions=param_dist,\n",
    "                        n_iter=n_iter_search)\n",
    "\n",
    "    start = time()\n",
    "    random_search.fit(X, y)\n",
    "    print((\"\\nRandomizedSearchCV took {:.2f} seconds \"\n",
    "           \"for {:d} candidates parameter \"\n",
    "           \"settings.\").format((time() - start),\n",
    "                               n_iter_search))\n",
    "\n",
    "    top_params = report(random_search.grid_scores_, 3)\n",
    "    return  top_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation without scaling nor normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_CV_fresh(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "    \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data \n",
    "                                    # and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "        \n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print(\"Root Mean Squared Logarithmic Error (higher; worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "    return pred\n",
    "\n",
    "\n",
    "def line_CV_fresh_para(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "    \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data \n",
    "                                    # and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "        \n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        print (\"Best parameters for fold \",ix,\"\\n\")\n",
    "        print (classifier.best_params_)\n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print (\"Root Mean Squared Logarithmic Error (higher-> worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation + scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_CV_spicy(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "\n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data \n",
    "                                    # and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "                \n",
    "        classifier.n_jobs = -1    \n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print (\"Root Mean Squared Logarithmic Error (higher; worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    return pred\n",
    "\n",
    "def line_CV_spicy_para(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "\n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data \n",
    "                                    # and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "                \n",
    "        classifier.n_jobs = -1    \n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        print (\"Best parameters for fold \",ix,\"\\n\")\n",
    "        print (classifier.best_params_)\n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print (\"Root Mean Squared Logarithmic Error (higher; worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation + scaling + normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_CV_very_spicy(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "    X_norm = preprocessing.normalize(design_matrix)\n",
    "  \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = X_norm[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = X_norm[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr)\n",
    "        Xte = scaler.transform(Xte)\n",
    "\n",
    "        classifier.n_jobs = -1\n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print (\"Root Mean Squared Logarithmic Error (higher; worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    return pred\n",
    "\n",
    "def line_CV_very_spicy_para(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "    X_norm = preprocessing.normalize(design_matrix)\n",
    "    \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = X_norm[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = X_norm[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr)\n",
    "        Xte = scaler.transform(Xte)\n",
    "\n",
    "        classifier.n_jobs = -1\n",
    "        classifier.fit(Xtr, ytr)\n",
    "        pred[te] = classifier.predict(Xte)\n",
    "        \n",
    "        print (\"Best parameters for fold \",ix,\"\\n\")\n",
    "        print (classifier.best_params_)\n",
    "        assert len(pred[te]) == len(labels[te])\n",
    "        RMSE = np.sqrt(np.mean(np.power(np.log1p(pred[te])-np.log1p(labels[te]), 2)))\n",
    "        print (\"Root Mean Squared Logarithmic Error (higher; worse): %0.3f\" % RMSE)\n",
    "        RMSE_ave[ix] = RMSE\n",
    "    \n",
    "    print (\"RMSLE Mean: %0.3f\" % np.mean(RMSE_ave))\n",
    "    print (\"\\n\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://inclass.kaggle.com/c/how-many-bikes/details/evaluation\n",
    "\n",
    "#### Kaggle Evaluation Method: Root Mean squared logarithmic error\n",
    "Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). This metric is suitable when predicting values across a large range of orders of magnitudes. It avoids penalizing large differences in prediction when both the predicted and the true number are large: predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545.\n",
    "\n",
    "Code-Reference from:\n",
    "https://ajourneyintodatascience.quora.com/Custom-evaluation-function-and-early-stopping-for-xgboost-with-k-fold-validation-Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation evaluated by mean squared error (MSE) & R Squared metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CV_with_MSE(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    RMSE_ave = np.zeros((len(cv_folds),1))\n",
    "    RS_ave = np.zeros((len(cv_folds),1))\n",
    "    MAPE_ave = np.zeros((len(cv_folds),1))\n",
    "    MASE_ave = np.zeros((len(cv_folds),1))\n",
    "    X_norm = preprocessing.normalize(design_matrix)\n",
    "    \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = X_norm[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = X_norm[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr)\n",
    "        Xte = scaler.transform(Xte)\n",
    "        \n",
    "        classifier.n_jobs = -1\n",
    "        classifier.fit(Xtr, ytr)\n",
    "        yte_pred = classifier.predict(Xte)\n",
    "        pred[te] = yte_pred[:]\n",
    "        \n",
    "        RMSE = mean_squared_error(labels[te], pred[te]) ** 0.5\n",
    "        RS = r2_score(labels[te], pred[te])\n",
    "        MAPE = np.mean(np.abs((labels[te] - pred[te]) / labels[te])) * 100\n",
    "        num = Xte.shape[0]\n",
    "        diffe = np.abs(  np.diff(Xte) ).sum()/(num-1)\n",
    "        errors = np.abs(pred[te] - labels[te])\n",
    "        MASE = errors.mean() / diffe\n",
    "        \n",
    "        print (\"Root Mean Squared Error: %0.3f, R Square mean: %0.3f\" % (RMSE, RS))\n",
    "        print (\"Mean absolute percentage error: %0.3f, Mean absolute scaled error: %0.3f\" % (MAPE, MASE))\n",
    "        RMSE_ave[ix] = RMSE\n",
    "        RS_ave[ix] = RS\n",
    "        MAPE_ave[ix] = MAPE\n",
    "        MASE_ave[ix] = MASE\n",
    "    \n",
    "    print (\"RMSE Mean: %0.3f, R Square mean: %0.3f\" % (np.mean(RMSE_ave), np.mean(RS_ave)))\n",
    "    print (\"MAPE: %0.3f, MASE: %0.3f\" % (np.mean(MAPE_ave), np.mean(MASE_ave)))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    return pred\n",
    "\n",
    "def CV_with_MSE_para(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape)\n",
    "    MSE_ave = np.zeros((len(cv_folds),1))\n",
    "    RS_ave = np.zeros((len(cv_folds),1))\n",
    "    MAPE_ave = np.zeros((len(cv_folds),1))\n",
    "    MASE_ave = np.zeros((len(cv_folds),1))\n",
    "    X_norm = preprocessing.normalize(design_matrix)\n",
    "    \n",
    "    for ix, (tr, te) in enumerate(cv_folds):\n",
    "        Xtr = X_norm[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = X_norm[te, :]\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr)\n",
    "        Xte = scaler.transform(Xte)\n",
    "           \n",
    "        classifier.n_jobs = -1\n",
    "        classifier.fit(Xtr, ytr)\n",
    "        pred[te] = classifier.predict(Xte)\n",
    "        \n",
    "        print (\"Best parameters for fold \",ix,\"\\n\")\n",
    "        print (classifier.best_params_)\n",
    "\n",
    "        MSE = mean_squared_error(labels[te], pred[te])\n",
    "        RS = r2_score(labels[te], pred[te])\n",
    "        MAPE = np.mean(np.abs((labels[te] - pred[te]) / labels[te])) * 100\n",
    "        num = Xte.shape[0]\n",
    "        diffe = np.abs(  np.diff(Xte) ).sum()/(num-1)\n",
    "        errors = np.abs(pred[te] - labels[te])\n",
    "        MASE = errors.mean() / diffe\n",
    "        \n",
    "        print (\"Root Mean Squared Error: %0.3f, R Square mean: %0.3f\" % (RMSE, RS))\n",
    "        print (\"Mean absolute percentage error: %0.3f, Mean absolute scaled error: %0.3f\" % (MAPE, MASE))\n",
    "        RMSE_ave[ix] = RMSE\n",
    "        RS_ave[ix] = RS\n",
    "        MAPE_ave[ix] = MAPE\n",
    "        MASE_ave[ix] = MASE\n",
    "    \n",
    "    print (\"RMSE Mean: %0.3f, & R Square mean: %0.3f\" % (np.mean(RMSE_ave), np.mean(RS_ave)))\n",
    "    print (\"MAPE: %0.3f, MASE: %0.3f\" % (np.mean(MAPE_ave), np.mean(MASE_ave)))\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part one: Run the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search parameter dictionary\n",
    "\n",
    "multiclass or multilabel problems -> average parameter\n",
    "\n",
    "\"macro\" \n",
    "simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "\"weighted\" \n",
    "accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n",
    "\n",
    "\"micro\" \n",
    "gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "\n",
    "\"samples\" \n",
    "applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n",
    "\n",
    "Selecting average=None will return an array with the score for each class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Grid Parameter Search via 10-fold CV\n",
      "\n",
      "GridSearchCV took 384.39 seconds for 6 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.013 (std: 0.056)\n",
      "Parameters: {'C': 10.0}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.013 (std: 0.052)\n",
      "Parameters: {'C': 100.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.010 (std: 0.062)\n",
      "Parameters: {'C': 1000.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print(\"-- Grid Parameter Search via 10-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "param_grid = {'C': [1e1,1e2,1e3,1e4,1e5,1e6]}\n",
    "\n",
    "LR_clf = linear_model.LogisticRegression()\n",
    "LR_ts_gs = run_gridsearch(X, y, LR_clf, param_grid, cv=10)\n",
    "\n",
    "Parameter Grid Search\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression cross validation without scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.339\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.292\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.173\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.264\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.046\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.131\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.270\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.125\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.252\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.177\n",
      "RMSLE Mean: 1.207\n",
      "\n",
      "\n",
      "linear regression cross validation with scaling:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.339\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.292\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.173\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.264\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.046\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.131\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.270\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.125\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.252\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.177\n",
      "RMSLE Mean: 1.207\n",
      "\n",
      "\n",
      "linear regression cross validation with scaling + normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.803\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.005\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.844\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.942\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.797\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.752\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.993\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.757\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.812\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.811\n",
      "RMSLE Mean: 0.852\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(C=1e2)\n",
    "\n",
    "print (\"linear regression cross validation without scaling and normalization:\")\n",
    "ypred_1 = line_CV_fresh(X, y, clf, kf)\n",
    "print (\"linear regression cross validation with scaling:\")\n",
    "ypred_2 = line_CV_spicy(X, y, clf, kf)\n",
    "print (\"linear regression cross validation with scaling + normalization:\")\n",
    "ypred_3 = line_CV_very_spicy(X, y, clf, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C parameter \n",
    "http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression \n",
    "\n",
    "\n",
    "### Reducing computational expense :  RandomizedSearchCV\n",
    "Searching many different parameters at once may be computationally infeasible\n",
    "  For example, Searching 10 parameters (each range of 1000)\n",
    "    (Require 10,000 trials of CV, 100,000 model fits with 10-fold CV, 100,000 predictions with 10-fold CV)\n",
    "\n",
    "http://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Grid Parameter Search via 10-fold CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thwowu/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Grid Parameter Search via 10-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "param_grid = {'C': [1e-3, 1e-2, 1., 1e-1, 1e2, 1e3]}\n",
    "\n",
    "LR_Reg_clf = linear_model.LogisticRegression(penalty='l1')\n",
    "LR_Reg_ts_gs = run_gridsearch(X, y, LR_Reg_clf, param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. L1-Regularized Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_ReL1 = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l1'), param_grid_regu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L2-regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_ReL2 = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l2'), param_grid_regu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regulizedlinear L1 regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.669\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.670\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.773\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.947\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.794\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.960\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 3.335\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.888\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.871\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.923\n",
      "RMSLE Mean: 2.883\n",
      "\n",
      "\n",
      "Regulized L2 linear regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.165\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.904\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.153\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.131\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.921\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.306\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.172\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.221\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.213\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.405\n",
      "RMSLE Mean: 2.159\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ReL1_set = linear_model.LogisticRegression(penalty='l1', C=1e-2)\n",
    "clf_ReL2_set = linear_model.LogisticRegression(penalty='l2', C=1e-2)\n",
    "\n",
    "print (\"Regulizedlinear L1 regression cross validation with scaling and normalization:\")\n",
    "ypred_1_regu = line_CV_very_spicy(X, y, clf_ReL1_set, kf)\n",
    "print (\"Regulized L2 linear regression cross validation with scaling and normalization:\")\n",
    "ypred_2_regu = line_CV_very_spicy(X, y, clf_ReL2_set, kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regulizedlinear L1 regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.405\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.491\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.905\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.867\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.408\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.763\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.815\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.838\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.786\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 3.079\n",
      "RMSLE Mean: 2.736\n",
      "\n",
      "\n",
      "Regulized L2 linear regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.094\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.798\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.996\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.975\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.708\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.204\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.006\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.074\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.115\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 2.296\n",
      "RMSLE Mean: 2.027\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ReL1_set = linear_model.LogisticRegression(penalty='l1', C=1e-1)\n",
    "clf_ReL2_set = linear_model.LogisticRegression(penalty='l2', C=1e-1)\n",
    "\n",
    "print (\"Regulizedlinear L1 regression cross validation with scaling and normalization:\")\n",
    "ypred_1_regu = line_CV_very_spicy(X, y, clf_ReL1_set, kf)\n",
    "print (\"Regulized L2 linear regression cross validation with scaling and normalization:\")\n",
    "ypred_2_regu = line_CV_very_spicy(X, y, clf_ReL2_set, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regulizedlinear L1 regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.554\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.523\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.413\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.507\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.262\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.521\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.449\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.554\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.576\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.747\n",
      "RMSLE Mean: 1.511\n",
      "\n",
      "\n",
      "Regulized L2 linear regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.696\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.591\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.491\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.620\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.415\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.647\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.573\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.595\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.677\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.814\n",
      "RMSLE Mean: 1.612\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ReL1_set = linear_model.LogisticRegression(penalty='l1', C=1.)\n",
    "clf_ReL2_set = linear_model.LogisticRegression(penalty='l2', C=1.)\n",
    "\n",
    "print (\"Regulizedlinear L1 regression cross validation with scaling and normalization:\")\n",
    "ypred_1_regu = line_CV_very_spicy(X, y, clf_ReL1_set, kf)\n",
    "print (\"Regulized L2 linear regression cross validation with scaling and normalization:\")\n",
    "ypred_2_regu = line_CV_very_spicy(X, y, clf_ReL2_set, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regulizedlinear L1 regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.900\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.927\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.864\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.895\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.815\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.798\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.923\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.759\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.784\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.845\n",
      "RMSLE Mean: 0.851\n",
      "\n",
      "\n",
      "Regulized L2 linear regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.086\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.994\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.022\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.061\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.954\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.083\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.183\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.124\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.129\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.214\n",
      "RMSLE Mean: 1.085\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ReL1_set = linear_model.LogisticRegression(penalty='l1', C=1e1)\n",
    "clf_ReL2_set = linear_model.LogisticRegression(penalty='l2', C=1e1)\n",
    "\n",
    "print (\"Regulizedlinear L1 regression cross validation with scaling and normalization:\")\n",
    "ypred_1_regu = line_CV_very_spicy(X, y, clf_ReL1_set, kf)\n",
    "print (\"Regulized L2 linear regression cross validation with scaling and normalization:\")\n",
    "ypred_2_regu = line_CV_very_spicy(X, y, clf_ReL2_set, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regulizedlinear L1 regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.828\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.896\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.868\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.896\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.835\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.699\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.917\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.802\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.867\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.780\n",
      "RMSLE Mean: 0.839\n",
      "\n",
      "\n",
      "Regulized L2 linear regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.803\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.005\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.844\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.942\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.797\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.752\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.993\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.757\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.812\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.811\n",
      "RMSLE Mean: 0.852\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ReL1_set = linear_model.LogisticRegression(penalty='l1', C=1e2)\n",
    "clf_ReL2_set = linear_model.LogisticRegression(penalty='l2', C=1e2)\n",
    "\n",
    "print (\"Regulizedlinear L1 regression cross validation with scaling and normalization:\")\n",
    "ypred_1_regu = line_CV_very_spicy(X, y, clf_ReL1_set, kf)\n",
    "print (\"Regulized L2 linear regression cross validation with scaling and normalization:\")\n",
    "ypred_2_regu = line_CV_very_spicy(X, y, clf_ReL2_set, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bypasss\n",
    "# export the prediction into csv file\n",
    "\n",
    "# yte_pred.tofile('foo.csv',sep=',',format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with Paramatric Grid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights:**\n",
    "default options --> uniform (all points in the neighborhood are weighted equally)\n",
    "another option --> distance (weights closer neighbors more heavily than further neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# paramatric grid\n",
    "metrics       = ['minkowski','euclidean','manhattan'] \n",
    "weights       = ['uniform','distance'] \n",
    "numNeighbors  = np.arange(1,30, 2)\n",
    "gamma_range   = np.logspace(-5, 5, 5) #10.0**np.arange(-5,4)\n",
    "param_dist    = dict(metric=metrics,weights=weights,n_neighbors=numNeighbors, gamma_range=gamma_range)\n",
    "\n",
    "# n_iter controls the number of searches\n",
    "# instantiate model\n",
    "# 2 new params\n",
    "# n_iter --> controls number of random combinations it will try\n",
    "# random_state for reproducibility \n",
    "rand = RandomizedSearchCV(neighbors.KNeighborsClassifier(),\n",
    "                          param_dist, cv=10, scoring='average_precision', n_iter=10, random_state=5)\n",
    "\n",
    "# fit\n",
    "rand.fit(X, y)\n",
    "\n",
    "# scores\n",
    "rand.grid_scores_\n",
    "\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)\n",
    "print(rand.best_estimator_)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- KNN Grid Parameter Search via 10-fold CV\n",
      "\n",
      "GridSearchCV took 72.96 seconds for 90 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.020 (std: 0.012)\n",
      "Parameters: {'n_neighbors': 5, 'metric': 'minkowski', 'weights': 'distance'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.020 (std: 0.007)\n",
      "Parameters: {'n_neighbors': 23, 'metric': 'minkowski', 'weights': 'distance'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.020 (std: 0.012)\n",
      "Parameters: {'n_neighbors': 5, 'metric': 'euclidean', 'weights': 'distance'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- KNN Grid Parameter Search via 10-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "metrics       = ['minkowski','euclidean','manhattan'] \n",
    "weights       = ['uniform','distance'] #10.0**np.arange(-5,4)\n",
    "numNeighbors  = np.arange(1,30, 2)\n",
    "param_dist    = dict(metric=metrics,weights=weights,n_neighbors=numNeighbors)       \n",
    "\n",
    "KNN_clf = neighbors.KNeighborsClassifier()\n",
    "KNN_ts_gs = run_gridsearch(X, y, KNN_clf, param_dist, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.692\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.632\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.589\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.546\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.583\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.612\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.619\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.598\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.577\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.639\n",
      "RMSLE Mean: 0.609\n",
      "\n",
      "\n",
      "Rank 2 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.617\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.701\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.655\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.614\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.664\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.695\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.629\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.637\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.658\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.618\n",
      "RMSLE Mean: 0.649\n",
      "\n",
      "\n",
      "Rank 3 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.692\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.632\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.589\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.546\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.583\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.612\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.619\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.598\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.577\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.639\n",
      "RMSLE Mean: 0.609\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rank 1:\n",
    "clf_Knn_set1 = neighbors.KNeighborsClassifier(metric = 'minkowski' ,\n",
    "                                             weights = 'distance',\n",
    "                                             n_neighbors = 5)\n",
    "#rank 2:\n",
    "clf_Knn_set2 = neighbors.KNeighborsClassifier(metric = 'minkowski',\n",
    "                                             weights = 'distance',\n",
    "                                             n_neighbors = 23)\n",
    "#rank 3:\n",
    "clf_Knn_set3 = neighbors.KNeighborsClassifier(metric =  'euclidean',\n",
    "                                             weights = 'distance',\n",
    "                                             n_neighbors = 5)\n",
    "\n",
    "print (\"Rank 1 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_1_knn = line_CV_very_spicy(X, y, clf_Knn_set1, kf)\n",
    "print (\"Rank 2 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_2_knn = line_CV_very_spicy(X, y, clf_Knn_set2, kf)\n",
    "print (\"Rank 3 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_3_knn = line_CV_very_spicy(X, y, clf_Knn_set3, kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"neigh = neighbors.KNeighborsClassifier(n_neighbors=clf_knn.best_params_)\n",
    "\n",
    "design_matrix = preprocessing.normalize(X)\n",
    "labels = y\n",
    "pred = np.zeros(z.shape)\n",
    "\n",
    "scaler = preprocessing.StandardScaler() # create scaler\n",
    "Xtr = design_matrix[:]\n",
    "ytr = labels[:]\n",
    "Xte = preprocessing.normalize(z)\n",
    "\n",
    "Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data and transform training data\n",
    "Xte = scaler.transform(Xte) # transform test data\n",
    "\n",
    "classifier.n_jobs = -1\n",
    "classifier.fit(Xtr, ytr)\n",
    "yte_pred2 = classifier.predict(Xte)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yte_pred2.tofile('foo10.csv',sep=',',format='%.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "### with \"gini\" and \"Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- KNN Grid Parameter Search via 5-fold CV\n",
      "\n",
      "GridSearchCV took 1313.51 seconds for 800 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.024 (std: 0.016)\n",
      "Parameters: {'min_samples_split': 20, 'max_leaf_nodes': 20, 'n_estimators': 10, 'criterion': 'gini', 'min_samples_leaf': 10}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.024 (std: 0.018)\n",
      "Parameters: {'min_samples_split': 3, 'max_leaf_nodes': 10, 'n_estimators': 20, 'criterion': 'entropy', 'min_samples_leaf': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.023 (std: 0.030)\n",
      "Parameters: {'min_samples_split': 10, 'max_leaf_nodes': 10, 'n_estimators': 15, 'criterion': 'gini', 'min_samples_leaf': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- KNN Grid Parameter Search via 5-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "n_estimators       = [5, 10, 15, 20]\n",
    "criterion          = [\"gini\", \"entropy\"]\n",
    "min_samples_split  = [1, 3, 5, 10, 20]\n",
    "min_samples_leaf   = [1, 3, 5, 10, 20]\n",
    "max_leaf_nodes     = [2, 5, 10, 20]\n",
    "param_grid_knn    = dict(n_estimators=n_estimators,\n",
    "                     criterion=criterion,\n",
    "                     min_samples_split=min_samples_split,\n",
    "                     min_samples_leaf=min_samples_leaf, \n",
    "                     max_leaf_nodes=max_leaf_nodes\n",
    "                    )      \n",
    "\n",
    "KNN_clf = ensemble.RandomForestClassifier()\n",
    "KNN_ts_gs = run_gridsearch(X, y, KNN_clf, param_grid_knn, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.058\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.095\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.934\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.961\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.848\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.832\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.905\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.858\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.060\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.163\n",
      "RMSLE Mean: 0.971\n",
      "\n",
      "\n",
      "Rank 2 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.900\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.323\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.224\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.239\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.923\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.105\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.103\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.119\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.048\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.392\n",
      "RMSLE Mean: 1.138\n",
      "\n",
      "\n",
      "Rank 3 KNN Regression cross validation with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.240\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.175\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.008\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.067\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.003\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.112\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.299\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.999\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.146\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.303\n",
      "RMSLE Mean: 1.135\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rank 1:\n",
    "clf_RF_set1 = ensemble.RandomForestClassifier(n_estimators  =  10,\n",
    "                                               criterion = 'gini',\n",
    "                                               min_samples_split = 20,\n",
    "                                               min_samples_leaf = 10, \n",
    "                                               max_leaf_nodes = 20)\n",
    "#rank 2:\n",
    "clf_RF_set2 = ensemble.RandomForestClassifier(n_estimators  =  20,\n",
    "                                               criterion = 'entropy',\n",
    "                                               min_samples_split = 3,\n",
    "                                               min_samples_leaf = 10, \n",
    "                                               max_leaf_nodes = 10)\n",
    "#rank 3:\n",
    "clf_RF_set3 = ensemble.RandomForestClassifier(n_estimators  =  15,\n",
    "                                               criterion = 'gini',\n",
    "                                               min_samples_split = 10,\n",
    "                                               min_samples_leaf = 3, \n",
    "                                               max_leaf_nodes = 10)\n",
    "\n",
    "print (\"Rank 1 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_1_RF = line_CV_very_spicy(X, y, clf_RF_set1, kf)\n",
    "print (\"Rank 2 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_2_RF = line_CV_very_spicy(X, y, clf_RF_set2, kf)\n",
    "print (\"Rank 3 KNN Regression cross validation with scaling and normalization:\")\n",
    "ypred_3_RF = line_CV_very_spicy(X, y, clf_RF_set3, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference:\n",
    "http://astrohackweek.org/blog/multi-output-random-forests.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "\n",
    "The only supported criterion is “mse” for the mean squared error.\n",
    "http://stackoverflow.com/questions/24803802/optimizing-randomforestregressor-for-other-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tried for 5 hours in 5 CV, running out of ram and no result\n",
    "\n",
    "print(\"-- Random Forest Regressor Grid Parameter Search via 5-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "n_estimators       = [10, 20, 50, 100, 1000]\n",
    "min_samples_split  = [1, 3, 5, 10, 20]\n",
    "min_samples_leaf   = [1, 3, 5, 10, 20]\n",
    "max_leaf_nodes     = [2, 5, 10, 20]\n",
    "param_grid_RFR    = dict(n_estimators=n_estimators,\n",
    "                     min_samples_split=min_samples_split,\n",
    "                     min_samples_leaf=min_samples_leaf, \n",
    "                     max_leaf_nodes=max_leaf_nodes\n",
    "                    )      \n",
    "\n",
    "RFR_clf = RandomForestRegressor()\n",
    "RFR_ts_gs = run_gridsearch(X, y, RFR_clf, param_grid_RFR, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-1 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.599\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.661\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.769\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.646\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.640\n",
      "RMSLE Mean: 0.663\n",
      "\n",
      "\n",
      "Rando-2 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.546\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.627\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.713\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.566\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.576\n",
      "RMSLE Mean: 0.606\n",
      "\n",
      "\n",
      "Rando-3 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.541\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.621\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.718\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.551\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.556\n",
      "RMSLE Mean: 0.597\n",
      "\n",
      "\n",
      "Rando-4 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.529\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.636\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.706\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.582\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.542\n",
      "RMSLE Mean: 0.599\n",
      "\n",
      "\n",
      "Rando-5 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.585\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.645\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.763\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.642\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.668\n",
      "RMSLE Mean: 0.661\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random 1:\n",
    "clf_RFR_set1 = ensemble.RandomForestClassifier(n_estimators  =  3000,\n",
    "                                               min_samples_split = 1,\n",
    "                                               min_samples_leaf = 3, \n",
    "                                               max_leaf_nodes = 1000)\n",
    "#random 2:\n",
    "clf_RFR_set2 = ensemble.RandomForestClassifier(n_estimators  =  3000,\n",
    "                                               min_samples_split = 3,\n",
    "                                               min_samples_leaf = 1, \n",
    "                                               max_leaf_nodes = 1000)\n",
    "#random 3:\n",
    "clf_RFR_set3 = ensemble.RandomForestClassifier(n_estimators  =  1000,\n",
    "                                               min_samples_split = 1,\n",
    "                                               min_samples_leaf = 1, \n",
    "                                               max_leaf_nodes = 500)\n",
    "#random 4:\n",
    "clf_RFR_set4 = ensemble.RandomForestClassifier(n_estimators  =  1000,\n",
    "                                               min_samples_split = 3,\n",
    "                                               min_samples_leaf = 1, \n",
    "                                               max_leaf_nodes = 500)\n",
    "#random 5:\n",
    "clf_RFR_set5 = ensemble.RandomForestClassifier(n_estimators  =  1000,\n",
    "                                               min_samples_split = 1,\n",
    "                                               min_samples_leaf = 3, \n",
    "                                               max_leaf_nodes = 500)\n",
    "\n",
    "print (\"Rando-1 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_1_RFR = line_CV_very_spicy(X, y, clf_RFR_set1, kf)\n",
    "print (\"Rando-2 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_2_RFR = line_CV_very_spicy(X, y, clf_RFR_set2, kf)\n",
    "print (\"Rando-3 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_3_RFR = line_CV_very_spicy(X, y, clf_RFR_set3, kf)\n",
    "print (\"Rando-4 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_4_RFR = line_CV_very_spicy(X, y, clf_RFR_set4, kf)\n",
    "print (\"Rando-5 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_5_RFR = line_CV_very_spicy(X, y, clf_RFR_set5, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-5 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.542\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.640\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.705\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.612\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.549\n",
      "RMSLE Mean: 0.610\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_RFR_set = ensemble.RandomForestClassifier(n_estimators  =  1000,\n",
    "                                               max_leaf_nodes = 500)\n",
    "print (\"Rando-5 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_RFR = line_CV_very_spicy(X, y, clf_RFR_set, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-5 Random Forest Regressor CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.392\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.404\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.447\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.421\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.445\n",
      "RMSLE Mean: 0.422\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_RFR_set6 = RandomForestRegressor(n_estimators  =  1000,\n",
    "                                     min_samples_split = 1,\n",
    "                                     min_samples_leaf = 1, \n",
    "                                     max_leaf_nodes = 500)\n",
    "print (\"Rando-5 Random Forest Regressor CV with scaling and normalization:\")\n",
    "ypred_RFR6 = line_CV_very_spicy(XX, yy, clf_RFR_set6, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_RFR_final = RandomForestRegressor(n_estimators  =  10000,\n",
    "                             min_samples_split = 1,\n",
    "                             min_samples_leaf = 1, \n",
    "                             max_leaf_nodes = 5000).fit(XX, yy)\n",
    "Ypred1 = clf_RFR_final.predict(zz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output as the CSV file (that no floating number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ypred1.tofile('foo21.csv',sep=',',format='%.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Support Vector Machine\n",
    "\n",
    "**degree** : int, optional (default=3)\n",
    "Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "\n",
    "**gamma** : float, optional (default=’auto’)\n",
    "Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "**coef0**: float, optional (default=0.0)\n",
    "Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"-- KNN Grid Parameter Search via 5-fold CV\")\n",
    "\n",
    "# set of parameters to test\n",
    "C                  = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "kernel             = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "degree             = [1, 2, 3, 4]\n",
    "gamma              = [0, 1, 2, 3]\n",
    "coef0              = [0, 1, 2, 3]\n",
    "random_state       = [None, 5, 20, 100, 300]\n",
    "param_grid_SVM    = dict(C=C,\n",
    "                     kernel=kernel,\n",
    "                     degree=degree,\n",
    "                     gamma=gamma, \n",
    "                     coef0=coef0, random_state = random_state\n",
    "                    )      \n",
    "\n",
    "SVM_clf = svm.SVC()\n",
    "SVM_ts_gs = run_gridsearch(X, y, SVM_clf, param_grid_SVM, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1: \n",
    "\n",
    "`clf_SVM_set1 = svm.SVC(C  =  0.1, kernel = 'poly', degree = 5, coef0 = 4) `\n",
    "\n",
    "2:\n",
    "\n",
    "`clf_SVM_set1 = svm.SVC(C  =  30, kernel = 'rbf')`\n",
    "\n",
    "3:\n",
    "\n",
    "`clf_SVM_set4 = svm.SVC(C  =  50, kernel = 'linear')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-1 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.877\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.739\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.757\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.752\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.780\n",
      "RMSLE Mean: 0.781\n",
      "\n",
      "\n",
      "Rando-2 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.674\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.608\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.600\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.587\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.638\n",
      "RMSLE Mean: 0.621\n",
      "\n",
      "\n",
      "Rando-3 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.651\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.620\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.585\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.585\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.617\n",
      "RMSLE Mean: 0.611\n",
      "\n",
      "\n",
      "Rando-4 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.645\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.608\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.589\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.586\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.597\n",
      "RMSLE Mean: 0.605\n",
      "\n",
      "\n",
      "Rando-5 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.635\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.594\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.587\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.591\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.610\n",
      "RMSLE Mean: 0.603\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random 1:\n",
    "clf_SVM_set1 = svm.SVC(C  =  1, kernel = 'linear')\n",
    "#random 2:\n",
    "clf_SVM_set2 = svm.SVC(C  =  10, kernel = 'linear')\n",
    "#random 3:\n",
    "clf_SVM_set3 = svm.SVC(C  =  30, kernel = 'linear')\n",
    "#random 4:\n",
    "clf_SVM_set4 = svm.SVC(C  =  50, kernel = 'linear')\n",
    "#random 5:\n",
    "clf_SVM_set5 = svm.SVC(C  =  70, kernel = 'linear')\n",
    "\n",
    "\n",
    "print (\"Rando-1 SVM CV with scaling and normalization:\")\n",
    "ypred_1_SVM = line_CV_very_spicy(X, y, clf_SVM_set1, kf)\n",
    "print (\"Rando-2 SVM CV with scaling and normalization:\")\n",
    "ypred_2_SVM = line_CV_very_spicy(X, y, clf_SVM_set2, kf)\n",
    "print (\"Rando-3 SVM CV with scaling and normalization:\")\n",
    "ypred_3_SVM = line_CV_very_spicy(X, y, clf_SVM_set3, kf)\n",
    "print (\"Rando-4 SVM CV with scaling and normalization:\")\n",
    "ypred_4_SVM = line_CV_very_spicy(X, y, clf_SVM_set4, kf)\n",
    "print (\"Rando-5 SVM CV with scaling and normalization:\")\n",
    "ypred_5_SVM = line_CV_very_spicy(X, y, clf_SVM_set5, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.cs.toronto.edu/~duvenaud/cookbook/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clf_SVM_set1 = svm.SVC(C  =  0.1, kernel = 'poly', degree = 6, coef0 = 4, `\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-1 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.332\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.370\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.384\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.423\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.370\n",
      "RMSLE Mean: 1.376\n",
      "\n",
      "\n",
      "Rando-2 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.287\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.332\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.343\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.379\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.333\n",
      "RMSLE Mean: 1.335\n",
      "\n",
      "\n",
      "Rando-3 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.067\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.095\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.094\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.121\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 1.115\n",
      "RMSLE Mean: 1.098\n",
      "\n",
      "\n",
      "Rando-4 SVM CV with scaling and normalization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thwowu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.902\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.921\n",
      "Root Mean Squared Logarithmic Error (higher; worse): 0.974\n",
      "RMSLE Mean: nan\n",
      "\n",
      "\n",
      "Rando-5 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "RMSLE Mean: nan\n",
      "\n",
      "\n",
      "Rando-6 SVM CV with scaling and normalization:\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "Root Mean Squared Logarithmic Error (higher; worse): nan\n",
      "RMSLE Mean: nan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random 1:\n",
    "clf_SVM_set1 = svm.SVC(C  =  30, kernel = 'rbf')\n",
    "#random 2:\n",
    "clf_SVM_set2 = svm.SVC(C  =  40, kernel = 'rbf')\n",
    "#random 3:\n",
    "clf_SVM_set3 = svm.SVC(C  =  50, kernel = 'rbf')\n",
    "#random 4:\n",
    "clf_SVM_set4 = svm.SVC(C  =  20, kernel = 'rbf')\n",
    "#random 5:\n",
    "clf_SVM_set5 = svm.SVC(C  =  25, kernel = 'rbf')\n",
    "#random 6:\n",
    "clf_SVM_set6 = svm.SVC(C  =  35, kernel = 'rbf')\n",
    "\n",
    "\n",
    "print (\"Rando-1 SVM CV with scaling and normalization:\")\n",
    "ypred_11_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set1, kf)\n",
    "print (\"Rando-2 SVM CV with scaling and normalization:\")\n",
    "ypred_22_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set2, kf)\n",
    "print (\"Rando-3 SVM CV with scaling and normalization:\")\n",
    "ypred_33_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set3, kf)\n",
    "print (\"Rando-4 SVM CV with scaling and normalization:\")\n",
    "ypred_44_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set4, kf)\n",
    "print (\"Rando-5 SVM CV with scaling and normalization:\")\n",
    "ypred_55_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set5, kf)\n",
    "print (\"Rando-6 SVM CV with scaling and normalization:\")\n",
    "ypred_66_SVM = line_CV_very_spicy(XX, yy, clf_SVM_set6, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "**loss: **\n",
    "‘deviance’, ‘exponential’\n",
    "\n",
    "**max_depth:**\n",
    "(default: 3)\n",
    "\n",
    "maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. **Tune this parameter** for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do we need to scale the data before doing GB? \n",
    "\n",
    "http://quant.stackexchange.com/questions/4434/gradient-tree-boosting-do-input-attributes-need-to-be-scaled\n",
    "\n",
    "Overall, you should probably do both (with and without). \n",
    "And recall, there are choices for scaling: \n",
    "\n",
    "(a) you can remove skewness (fat-tails)\n",
    "\n",
    "(b) mean-zero standardize\n",
    "\n",
    "(c) normalize in range [0,1]\n",
    "\n",
    "(d) determine direction cosines of the input features so feature values are within a unit circle.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://people.duke.edu/~rnau/compare.htm\n",
    "\n",
    "\n",
    "**The mean absolute scaled error (MASE)** is another relative measure of error that is applicable only to time series data.  It is defined as the mean absolute error of the model divided by the mean absolute error of a naïve random-walk-without-drift model (i.e., the mean absolute value of the first difference of the series).  Thus, it measures the relative reduction in error compared to a naive model. **Ideally its value will be significantly less than 1. ** This statistic, which was proposed by Rob Hyndman in 2006, is very good to look at when fitting regression models to nonseasonal time series data.  It is possible for a time series regression model to have an impressive R-squared and yet be inferior to a naïve model, as was demonstrated in the what’s-a-good-value-for-R-squared notes.  If the series has a strong seasonal pattern, the corresponding statistic to look at would be the mean absolute error divided by the mean absolute value of the seasonal difference (i.e., the mean absolute error of a naïve seasonal model that predicts that the value in a given period will equal the value observed one season ago).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rando-1 Gradient Boosting\n",
      "Root Mean Squared Error: 39.013, R Square mean: 0.955\n",
      "Mean absolute percentage error: 31.970, Mean absolute scaled error: 1.503\n",
      "Root Mean Squared Error: 39.896, R Square mean: 0.951\n",
      "Mean absolute percentage error: 32.135, Mean absolute scaled error: 1.509\n",
      "Root Mean Squared Error: 37.768, R Square mean: 0.956\n",
      "Mean absolute percentage error: 27.201, Mean absolute scaled error: 1.426\n",
      "Root Mean Squared Error: 41.238, R Square mean: 0.948\n",
      "Mean absolute percentage error: 30.297, Mean absolute scaled error: 1.530\n",
      "Root Mean Squared Error: 41.750, R Square mean: 0.947\n",
      "Mean absolute percentage error: 29.241, Mean absolute scaled error: 1.539\n",
      "RMSE Mean: 39.933, R Square mean: 0.951\n",
      "MAPE: 30.169, MASE: 1.501\n",
      "\n",
      "\n",
      "Rando-2 Gradient Boosting\n",
      "Root Mean Squared Error: 39.156, R Square mean: 0.954\n",
      "Mean absolute percentage error: 31.936, Mean absolute scaled error: 1.506\n",
      "Root Mean Squared Error: 39.803, R Square mean: 0.951\n",
      "Mean absolute percentage error: 32.597, Mean absolute scaled error: 1.506\n",
      "Root Mean Squared Error: 37.731, R Square mean: 0.957\n",
      "Mean absolute percentage error: 27.111, Mean absolute scaled error: 1.425\n",
      "Root Mean Squared Error: 41.149, R Square mean: 0.948\n",
      "Mean absolute percentage error: 30.238, Mean absolute scaled error: 1.525\n",
      "Root Mean Squared Error: 41.663, R Square mean: 0.947\n",
      "Mean absolute percentage error: 29.212, Mean absolute scaled error: 1.537\n",
      "RMSE Mean: 39.901, R Square mean: 0.951\n",
      "MAPE: 30.219, MASE: 1.500\n",
      "\n",
      "\n",
      "Rando-3 Gradient Boosting\n",
      "Root Mean Squared Error: 39.478, R Square mean: 0.954\n",
      "Mean absolute percentage error: 31.368, Mean absolute scaled error: 1.513\n",
      "Root Mean Squared Error: 40.124, R Square mean: 0.950\n",
      "Mean absolute percentage error: 31.475, Mean absolute scaled error: 1.520\n",
      "Root Mean Squared Error: 38.265, R Square mean: 0.955\n",
      "Mean absolute percentage error: 27.898, Mean absolute scaled error: 1.439\n",
      "Root Mean Squared Error: 41.518, R Square mean: 0.947\n",
      "Mean absolute percentage error: 30.724, Mean absolute scaled error: 1.549\n",
      "Root Mean Squared Error: 42.854, R Square mean: 0.944\n",
      "Mean absolute percentage error: 30.404, Mean absolute scaled error: 1.579\n",
      "RMSE Mean: 40.448, R Square mean: 0.950\n",
      "MAPE: 30.374, MASE: 1.520\n",
      "\n",
      "\n",
      "Rando-4 Gradient Boosting\n",
      "Root Mean Squared Error: 40.669, R Square mean: 0.951\n",
      "Mean absolute percentage error: 36.278, Mean absolute scaled error: 1.557\n",
      "Root Mean Squared Error: 41.183, R Square mean: 0.948\n",
      "Mean absolute percentage error: 34.883, Mean absolute scaled error: 1.580\n",
      "Root Mean Squared Error: 38.388, R Square mean: 0.955\n",
      "Mean absolute percentage error: 29.957, Mean absolute scaled error: 1.495\n",
      "Root Mean Squared Error: 43.252, R Square mean: 0.942\n",
      "Mean absolute percentage error: 31.782, Mean absolute scaled error: 1.615\n",
      "Root Mean Squared Error: 43.526, R Square mean: 0.942\n",
      "Mean absolute percentage error: 32.847, Mean absolute scaled error: 1.631\n",
      "RMSE Mean: 41.404, R Square mean: 0.948\n",
      "MAPE: 33.149, MASE: 1.576\n",
      "\n",
      "\n",
      "Rando-5 Gradient Boosting\n",
      "Root Mean Squared Error: 39.173, R Square mean: 0.954\n",
      "Mean absolute percentage error: 32.685, Mean absolute scaled error: 1.509\n",
      "Root Mean Squared Error: 39.910, R Square mean: 0.951\n",
      "Mean absolute percentage error: 32.604, Mean absolute scaled error: 1.511\n",
      "Root Mean Squared Error: 37.657, R Square mean: 0.957\n",
      "Mean absolute percentage error: 27.291, Mean absolute scaled error: 1.423\n",
      "Root Mean Squared Error: 41.163, R Square mean: 0.948\n",
      "Mean absolute percentage error: 29.855, Mean absolute scaled error: 1.529\n",
      "Root Mean Squared Error: 41.765, R Square mean: 0.947\n",
      "Mean absolute percentage error: 29.379, Mean absolute scaled error: 1.544\n",
      "RMSE Mean: 39.934, R Square mean: 0.951\n",
      "MAPE: 30.363, MASE: 1.503\n",
      "\n",
      "\n",
      "Rando-6 Gradient Boosting\n",
      "Root Mean Squared Error: 39.062, R Square mean: 0.955\n",
      "Mean absolute percentage error: 31.158, Mean absolute scaled error: 1.508\n",
      "Root Mean Squared Error: 40.028, R Square mean: 0.950\n",
      "Mean absolute percentage error: 31.054, Mean absolute scaled error: 1.500\n",
      "Root Mean Squared Error: 38.949, R Square mean: 0.954\n",
      "Mean absolute percentage error: 29.242, Mean absolute scaled error: 1.475\n",
      "Root Mean Squared Error: 42.614, R Square mean: 0.944\n",
      "Mean absolute percentage error: 30.125, Mean absolute scaled error: 1.577\n",
      "Root Mean Squared Error: 40.392, R Square mean: 0.950\n",
      "Mean absolute percentage error: 29.198, Mean absolute scaled error: 1.517\n",
      "RMSE Mean: 40.209, R Square mean: 0.951\n",
      "MAPE: 30.155, MASE: 1.515\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random 1:\n",
    "clf_GB_set1 = ensemble.GradientBoostingRegressor(n_estimators = 800, \n",
    "                                                 max_depth = 7, \n",
    "                                                 learning_rate = 0.05)\n",
    "#random 2:\n",
    "clf_GB_set2 = ensemble.GradientBoostingRegressor(n_estimators = 1000, \n",
    "                                                 max_depth = 7, \n",
    "                                                 learning_rate = 0.05)\n",
    "#random 3:\n",
    "clf_GB_set3 = ensemble.GradientBoostingRegressor(n_estimators = 1200, \n",
    "                                                 max_depth = 7, \n",
    "                                                 learning_rate = 0.1)\n",
    "#random 4:\n",
    "clf_GB_set4 = ensemble.GradientBoostingRegressor(n_estimators = 1500, \n",
    "                                                 max_depth = 7, \n",
    "                                                 learning_rate = 0.2)\n",
    "#random 5:\n",
    "clf_GB_set5 = ensemble.GradientBoostingRegressor(n_estimators = 1800, \n",
    "                                                 max_depth = 7, \n",
    "                                                 learning_rate = 0.05)\n",
    "#random 6:\n",
    "clf_GB_set6 = ensemble.GradientBoostingRegressor(n_estimators = 2000, \n",
    "                                                 max_depth = 7, \n",
    "                                                 loss = 'huber', \n",
    "                                                 learning_rate = 0.03)\n",
    "\n",
    "\n",
    "print (\"Rando-1 Gradient Boosting\")\n",
    "ypred_11_GB = CV_with_MSE(XX, yy, clf_GB_set1, kf)\n",
    "print (\"Rando-2 Gradient Boosting\")\n",
    "ypred_22_GB = CV_with_MSE(XX, yy, clf_GB_set2, kf)\n",
    "\n",
    "print (\"Rando-3 Gradient Boosting\")\n",
    "ypred_33_GB = CV_with_MSE(XX, yy, clf_GB_set3, kf)\n",
    "print (\"Rando-4 Gradient Boosting\")\n",
    "ypred_44_GB = CV_with_MSE(XX, yy, clf_GB_set4, kf)\n",
    "print (\"Rando-5 Gradient Boosting\")\n",
    "ypred_55_GB = CV_with_MSE(XX, yy, clf_GB_set5, kf)\n",
    "print (\"Rando-6 Gradient Boosting\")\n",
    "ypred_66_GB = CV_with_MSE(XX, yy, clf_GB_set6, kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.grid_search import GridSearchCV as GS\n",
    "from sklearn import ensemble\n",
    "\n",
    "\"\"\"num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_validation.cross_val_score(model, X, y, cv=kfold)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extreme Boosting\n",
    "\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "\n",
    "\n",
    "http://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
